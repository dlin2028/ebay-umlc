{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_titles = pd.read_csv('./dataset/Train_Tagged_Titles.tsv', sep='\\t', on_bad_lines='skip', quoting=csv.QUOTE_NONE, encoding='utf8')\n",
    "\n",
    "valid_tags = ['Accents', 'Brand', 'Character', 'Character Family', 'Closure', 'Color', 'Country/Region of Manufacture', 'Department', 'Fabric Type', 'Features', 'Handle Drop', 'Handle Style', 'Handle/Strap Material', 'Hardware Material', 'Lining Material', 'MPN', 'Material', 'Measurement, Dimension', 'Model', 'Occasion', 'Pattern', 'Pocket Type', 'Product Line', 'Season', 'Size', 'Strap Drop', 'Style', 'Theme', 'Trim Material', 'Type']\n",
    "\n",
    "tokens = tagged_titles.groupby('Record Number')['Token'].apply(list).to_dict()\n",
    "tags = tagged_titles.groupby('Record Number')['Tag'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenized_data = [[(tokens[i][tiddie], tags[i][tiddie]) for tiddie in range(0, len(tags[i]))] for i in range(1, len(tags) + 1)]\n",
    "\n",
    "#Append NaN token to previous value\n",
    "#ex. append \"Vuitton\" to \"Louis\" in \"Louis Vuitton\"\n",
    "for i in range(0, len(raw_tokenized_data)):\n",
    "    for j in reversed(range(1, len(raw_tokenized_data[i]))):\n",
    "        if (raw_tokenized_data[i][j][1] != raw_tokenized_data[i][j][1]): #python nan moment\n",
    "            raw_tokenized_data[i][j - 1] = (raw_tokenized_data[i][j - 1][0] + \" \" + raw_tokenized_data[i][j][0], raw_tokenized_data[i][j - 1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the NaN values\n",
    "trimmed_tokenized_data = [[i for i in item if not i[1] != i[1]] for item in raw_tokenized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reinitialize tokens and tags\n",
    "tokens = [' '.join([i[0] for i in item]) for item in trimmed_tokenized_data]\n",
    "tags = [[i[1] for i in item] for item in trimmed_tokenized_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from\n",
    "# [token, token, ...]\n",
    "# [tag, tag, ...]\n",
    "# to \n",
    "# [[title, [(begin, end, token), (begin, end, token), ...]], [title, [(begin, end, token), (begin, end, token), ...]], ...]\n",
    "for i in range(0, len(tags)):\n",
    "    curr = 0\n",
    "    for j in range(0, len(tags[i])):\n",
    "        tags[i][j] = (curr, curr + len(trimmed_tokenized_data[i][j][0]), tags[i][j])\n",
    "        curr += len(trimmed_tokenized_data[i][j][0]) + 1\n",
    "#this stays\n",
    "amouranth = [(tokens[i], tags[i]) for i in range(0, len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add valid tags\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for tag in valid_tags:\n",
    "    ner.add_label(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and dev\n",
    "dev = amouranth[::10]\n",
    "train = amouranth\n",
    "del train[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 4500 lines to ./trainset/train.spacy\n",
      "wrote 500 lines to ./trainset/dev.spacy\n"
     ]
    }
   ],
   "source": [
    "#output data\n",
    "os.makedirs(\"./trainset\", exist_ok=True)\n",
    "\n",
    "db = DocBin()\n",
    "for text, annotations in train:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if(span is None):\n",
    "            print(\"skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./trainset/train.spacy\")\n",
    "print(\"wrote \" + str(len(train)) + \" lines to ./trainset/train.spacy\")\n",
    "\n",
    "db = DocBin()\n",
    "for text, annotations in dev:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if(span is None):\n",
    "            print(\"skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./trainset/dev.spacy\")\n",
    "print(\"wrote \" + str(len(dev)) + \" lines to ./trainset/dev.spacy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('poopy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f52ab0dcc20aaef27cce08a77f09e222b8b3949b76637bac784c92d7f6a2f51c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
