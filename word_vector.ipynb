{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88b9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import multiprocessing\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4f3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_titles = pd.read_csv('./dataset/Listing_Titles.tsv', sep='\\t', on_bad_lines='skip', quoting=csv.QUOTE_NONE, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1295c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['louis vuitton',\n",
       "  'm40096',\n",
       "  'handbag',\n",
       "  'priscilla',\n",
       "  'multi-color',\n",
       "  'canvas',\n",
       "  'multi-color',\n",
       "  'canvas'],\n",
       " ['louis vuitton',\n",
       "  'petit noe',\n",
       "  'drawstring',\n",
       "  'shoulder',\n",
       "  'bag',\n",
       "  'monogram',\n",
       "  'leather',\n",
       "  'm42226',\n",
       "  '39sd442'],\n",
       " ['louis vuitton',\n",
       "  'damier',\n",
       "  'azur',\n",
       "  'pochette',\n",
       "  'bosphore',\n",
       "  'shoulder',\n",
       "  'bag',\n",
       "  'n51112',\n",
       "  'lv',\n",
       "  'auth',\n",
       "  'yt523'],\n",
       " ['gucci',\n",
       "  'bamboo',\n",
       "  '2way',\n",
       "  'shoulder',\n",
       "  'bag',\n",
       "  'leather',\n",
       "  'brown',\n",
       "  'auth',\n",
       "  'fm1002'],\n",
       " ['rank',\n",
       "  'ab',\n",
       "  'vintage',\n",
       "  'gucci',\n",
       "  'sherry line',\n",
       "  'pvc leather',\n",
       "  'clutch',\n",
       "  'bag',\n",
       "  'brown',\n",
       "  'from',\n",
       "  'japan',\n",
       "  'a128'],\n",
       " ['1970s',\n",
       "  'nyc',\n",
       "  'bonnie cashin',\n",
       "  'coach',\n",
       "  'brown',\n",
       "  'gray',\n",
       "  'leather',\n",
       "  'saddle',\n",
       "  'pouch',\n",
       "  'crossbody',\n",
       "  'bag'],\n",
       " ['louis vuitton',\n",
       "  'epi',\n",
       "  'serviette fermoir',\n",
       "  'business',\n",
       "  'bag',\n",
       "  'brown',\n",
       "  'm54358',\n",
       "  'lv',\n",
       "  'auth',\n",
       "  'gt2071'],\n",
       " ['womens', 'handbag'],\n",
       " ['chanel', 'leo lion', 'flap', 'bag', 'chevron', 'lambskin', 'medium'],\n",
       " ['gucci',\n",
       "  'white',\n",
       "  '\"',\n",
       "  'hobo',\n",
       "  '\"',\n",
       "  'vintage',\n",
       "  'handbag',\n",
       "  'authenticated',\n",
       "  'two',\n",
       "  'tone',\n",
       "  '\"',\n",
       "  'genuine',\n",
       "  'leather',\n",
       "  '\"']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_titles = pd.read_csv('./dataset/Train_Tagged_Titles.tsv', sep='\\t', on_bad_lines='skip', quoting=csv.QUOTE_NONE, encoding='utf8')\n",
    "\n",
    "valid_tags = ['Accents', 'Brand', 'Character', 'Character Family', 'Closure', 'Color', 'Country/Region of Manufacture', 'Department', 'Fabric Type', 'Features', 'Handle Drop', 'Handle Style', 'Handle/Strap Material', 'Hardware Material', 'Lining Material', 'MPN', 'Material', 'Measurement, dimension', 'Model', 'Occasion', 'Pattern', 'Pocket Type', 'Product Line', 'Season', 'Size', 'Strap Drop', 'Style', 'Theme', 'Trim Material', 'Type']\n",
    "\n",
    "tokens = tagged_titles.groupby('Record Number')['Token'].apply(list).to_dict()\n",
    "tags = tagged_titles.groupby('Record Number')['Tag'].apply(list).to_dict()\n",
    "texts = tagged_titles.groupby('Record Number')['Title'].apply(list).to_dict()\n",
    "\n",
    "tokens = tagged_titles.groupby('Record Number')['Token'].apply(list).to_dict()\n",
    "tags = tagged_titles.groupby('Record Number')['Tag'].apply(list).to_dict()\n",
    "raw_tokenized_data = [[(tokens[i][tiddie], tags[i][tiddie]) for tiddie in range(0, len(tags[i]))] for i in range(1, len(tags) + 1)]\n",
    "\n",
    "#Append NaN token to previous value\n",
    "#ex. append \"Vuitton\" to \"Louis\" in \"Louis Vuitton\"\n",
    "for i in range(0, len(raw_tokenized_data)):\n",
    "    for j in reversed(range(1, len(raw_tokenized_data[i]))):\n",
    "        if (raw_tokenized_data[i][j][1] != raw_tokenized_data[i][j][1]): #python nan moment\n",
    "            raw_tokenized_data[i][j - 1] = (raw_tokenized_data[i][j - 1][0] + \" \" + raw_tokenized_data[i][j][0], raw_tokenized_data[i][j - 1][1])\n",
    "\n",
    "trimmed_tokenized_data = [[i for i in item if not i[1] != i[1]] for item in raw_tokenized_data]\n",
    "\n",
    "trimmed_tokenized_data\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for i in range(0, len(trimmed_tokenized_data)):\n",
    "    name = []\n",
    "    for j in range(0, len(trimmed_tokenized_data[i])):\n",
    "        name.append(trimmed_tokenized_data[i][j][0].lower())\n",
    "    sentences.append(name)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064390dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Made Phrases\n",
      "Made Bigrams\n",
      "Found sentences\n",
      "63\n",
      "Training model now...\n",
      "[('hobo', 0.2650150954723358), ('lv', 0.2151338905096054), ('auth', 0.19011564552783966), ('saddle', 0.17705972492694855), ('m40096', 0.1452275812625885), ('handbag', 0.1442415565252304), ('gray', 0.14372088015079498), ('petit noe', 0.09479113668203354), ('yt523', 0.08924536406993866), ('sherry line', 0.08247613161802292)]\n"
     ]
    }
   ],
   "source": [
    "def create_wordvecs(corpus, model_name):\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "    from gensim.models.phrases import Phrases, Phraser\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    print (len(corpus))\n",
    "    \n",
    "\n",
    "    phrases = Phrases(corpus, min_count=30, progress_per=10000)\n",
    "    print (\"Made Phrases\")\n",
    "    \n",
    "    bigram = Phraser(phrases)\n",
    "    print (\"Made Bigrams\")\n",
    "    \n",
    "    sentences = phrases[corpus]\n",
    "    print (\"Found sentences\")\n",
    "    word_freq = defaultdict(int)\n",
    "\n",
    "    for sent in sentences:\n",
    "        for i in sent:\n",
    "            word_freq[i]+=1\n",
    "\n",
    "    print (len(word_freq))\n",
    "    \n",
    "    print (\"Training model now...\")\n",
    "    w2v_model = Word2Vec(min_count=1,\n",
    "                        window=2,\n",
    "                        sample=6e-5,\n",
    "                        alpha=0.03,\n",
    "                        min_alpha=0.0007,\n",
    "                        negative=20)\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "    print(w2v_model.wv.most_similar(positive = [\"chanel\"]))\n",
    "\n",
    "\n",
    "    os.makedirs(\"./trainset\", exist_ok=True)\n",
    "    w2v_model.wv.save_word2vec_format(f\"trainset/{model_name}.txt\")\n",
    "create_wordvecs(sentences, \"word_vecs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('poopy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "adf12df7b235022e4c7536daceb7bedbd14b85006e9114754938519bb40d111a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
